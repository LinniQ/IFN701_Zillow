 ---
title: "IFN701: Zillow's Home Value Prediction (Zestimate) --- Round 1 Logerrow Prediction"
author: "Linni, QIN n9632981"
output:
  pdf_document: default
  html_document: default
---

## Introduction

This R markdown file is going to find out the logerror of Zestimate in Oct. Nov. Dec. of both 2016 and 2017, which is associated to 3millions of properties in three counties in California, USA (LA, OR, VA). Logerror is the difference between the Zestimate's preicted market value and the actual sales price of the particular property. 

(Note: Detail introcution can be referred to Project Proposal)

Official data from Kaggle:
train_2016_v2.csv contatins the logerror of around 90thounds of properties sold within three specific counties through 2016.
properties_2016.csv includes the total 3millions of properties within three specific counties with common 57 attributes such as room numbers, square footage and locations.

Outcome:
1. data analysis report: find out the valuable data from the collected samples
eg.
    the pattern of the sales
    the pattern of the error distribution
    
    the features that contains nearly full values matching all parcelids (28 features)
    --- 29 features contain less than 70% of values for the parcelides (deleted)
    
    the factors to influcence the home value:
    --- transaction counts: cor (-0.7130957)
    
2. Prediction model
eg.
    logistic regression to predict the logerror based on the strongest influencial features

Prolem solving Methods:

...


# Import the available data

```{r,fig.width=10, fig.height=7}

train16 <- read.csv("./train_2016_v2.csv", header = TRUE) # "fread()" is a function to read large file and creat the table faster and more convenient.
prop16 <- read.csv("./properties_2016.csv", header = TRUE)
```


## 1. Data Analysis


First of all, let's explore the Error rate, Transaction pattern and Logerror Distribution.


```{r}
# convert the date into the name of the month for further visulation analysis
train16$month <- months(as.Date(train16$transactiondate, format = "%d/%m/%y"))
head(train16)

```

# 1.1 The accuracy of Zestimate (https://www.zillow.com/zestimate/) 

    "Zillow's accuracy has a median error rate of 5%." 
    
    Is the median error value cloaser to their offical error rate?
    0.006 = 0.6% ?

```{r}
library(plyr)

summarise(train16, Median=median(train16$logerror), Mean= mean(train16$logerror), Max=max(train16$logerror), Min=min(train16$logerror), Std=sd(train16$logerror)) 

```


# 1.2 Pattern of Transaction Counts of Zillow in 2016.

```{r}

tempA <- ddply(train16, .(month), "nrow") 
names(tempA)[2] <- c("transactioncount")
tempA$month <- ordered(tempA$month, levels = c("December","November","October","September", "August", "July","June", "May", "April", "March",   "February","January" ))

library(ggplot2)
ggplot(tempA) + geom_bar(aes(x= month, y=transactioncount), fill = "pink", stat = "identity") + xlab("Months") + ylab("Transaction Counts of Properties on Zillow (2016)")+ scale_y_continuous(limits = c(0, 11500), breaks = seq(0, 11500, 1000)) + geom_hline(yintercept = mean(tempA$transactioncount),linetype = "dashed", size = 2, color = "brown") + coord_flip() + theme_bw()
```

The brown dashed line is the mean of the transaction time of Zillow in 2016. It can be learned from the diagram that it was a cold market from October to December. Their volume is far lower than the mean of transaction counts.  

# 1.3 Logerror Distribution pattern of Zillow in 2016

1.3.1 Distribution of Logerror

Try jitter plot & box plot.
```{r,fig.width=10, fig.height=7}

train16[which(train16$logerror== max(train16$logerror)),] # to determine the range of scale for later plotting
train16[which(train16$logerror== min(train16$logerror)),]

```

```{r,fig.width=10, fig.height=7}

train16$month <- ordered(train16$month, levels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October","November", "December"))

ggplot(train16) + geom_jitter(aes(x=month, y=logerror), stat = "identity", color = "brown") + xlab ("Months (2016)") + ylab("Logerror Distribution") + scale_y_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 0.25)) + geom_hline(yintercept = 0 ,linetype = "dashed", size = 2, color = "black")+ theme_bw()


```


Try the visulization in boxplot. Jitter is the better way to display the error distribution after comparing the graph generated by geom_jitter and geom_boxplot.

```{r,fig.width=10, fig.height=7}

ggplot(train16,aes(x=month, y=logerror)) + geom_boxplot(color="brown") + xlab ("Months (2016)") + ylab("Logerror Distribution")+ scale_y_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 0.25))+ theme_bw()
```





1.3.2 Relationship between Monthly Logerror Mean and the Monthly Transaction Counts

```{r,fig.width=10, fig.height=7}

tempB <- ddply(train16, c("month"), summarise, mean = mean(logerror))
tempB$month <- ordered(tempB$month, levels = c("January", "February", "March","April","May", "June", "July","August","September",  "October","November", "December"))

# coef(lm(median ~ month, data = tempB ))
library(ggplot2)
ggplot(tempB, aes(x=month, y=mean, group=1)) + geom_point(color="brown", size=5) + geom_line(color="brown", size=1.5) + xlab ("Month (2016)") + ylab("Logerror (Brown) and Transaction Counts (Purple)") + scale_y_continuous(limits = c(0, 0.02), breaks = seq(0, 0.02, 0.002)) + geom_line(aes(x=tempA$month,y=tempA$transactioncount/10^6), color = "purple", size = 1.5) +theme_bw()

```

As indicated as the above graph, the brown line represents the mean of error of each month and the purple line denotes the transaction times that is being sclaled by dividing 10^6. Considering the distribution of the error, the high logerrors happened among January, February, September, October, November and December of 2016 when the transaction times stayed low. A stable flutuation of the error happened from March to August when the transaction times stayed high. These two variables run in the opposite direction as shown. Let's check their correlation rate by using cor().


```{r,fig.width=10, fig.height=7}
tempAB <- merge(tempA, tempB, by = "month")

cor(tempAB$mean, tempAB$transactioncount/10^6)

```

In staticstics, the correlation is represented within the range of -1, 0 and 1. 0 indicates no correlation and 1 denotes a positive correlation.

The negative correlation as above between the logerror and the transaction numbers denotes while transaction number increases the logeeror decreases. (http://www.investopedia.com/terms/n/negative-correlation.asp)

Question: Can we use the transanction time to predict the error?

There is data of 2016 only for training. It is lack of strong evidence to prove that the transaction times stay low in the last season of every year. It means the error cannot be predicted based only on the transaction.



# 1.4  Coorelations among 57 home attributes 

According to Zillow, the accuracy of Zestimate depands on the available data supplied by the homeowners. So, does that mean:
January, February, September, October, November and December of 2016 = less home feature value?
March, April, May, June, July and August of 2016 = enough home feature value?
Let's combine prop16 and train16 by parcelid. The result file will tell us the physical features of the sold property in 2016 and their respective error. So, the relationships hidden among those factors might be disclosure.

# 1.4.1 Join property data and train data

```{r}
install.packages("dplyr")

library(dplyr)
joinid <- semi_join(prop16, train16, by = "parcelid") # There are 90150 properties in the total sample are sold in 2016,


mergeid <- merge (x=train16, y=joinid, by = "parcelid", all.y = TRUE) # wierd????why doesnot the shape show 90150,61 but 90275,61. The other 125 properties should not contain the attributes values theoritically. 

#mergeid$month <- ordered(mergeid$month, levels = c("January", "February", "March","April","May", "June", "July","August","September",  "October","November", "December")) # I want to order the data according to month order, but it does not work.

#antij <- anti_join(x = train16, y= joinid, by = "parcelid", all.x=TRUE) # why (0,4)? it should be (125,x)..Cannot find the way to produce  90150 + 61v to improve the accuracy of the merge file. Will try if needed later.

```


# 1.4.2 check the missing data for each field

```{r,fig.width=10, fig.height=20}
#sum(!is.na(joinid$airconditioningtypeid)) # count the row number of column with values


#tempF1 <- ddply(joinid, .(airconditioningtypeid), "nrow") # count row number for each unique value in the column

#apply(joinid, 2, function(x) length(which(!is.na(x)))) # same results with colSums(!is.na(joinid))

tempF <- data.frame(colSums(!is.na(joinid)))
names(tempF)[1] <- c("values")


ggplot(tempF,aes(x=reorder(rownames(tempF),-values), y=values)) + geom_bar( fill = "pink", stat = "identity") + xlab("Property Physical Features (2016)") + ylab("Available values Counts for Featrues")+ scale_y_continuous(limits = c(0, 91000), breaks = seq(0, 91000, 5000)) +coord_flip() + theme_bw()


```

```{r}
subset(tempF,rownames(tempF)=="unitcnt") #58721 

```
As we can see, from the top to unicnt, there are 29 (50%) property features that were supplied values for less than 70% of the total amount of sold parcelids.

Let us narrow down the size of the dataset and focus on figuring out the relationship between the estimate error and the feature variables with full values.

# 1.4.3 Clean the data of joinid (90150 sold parcelids), out = mertempF2df

```{r}
tempF1 <- subset(tempF,tempF > 58721) 
nrow(tempF1) #29, including parcelid

```
There are 29 features that contain nealy full values matching 90150 sold parcelid, including parcelid. They are listed in tempF1.

```{r}

tempF2 <- subset(tempF,tempF == 90150) # there are 15
nrow(tempF2) #16, including parcelid
```

There are 16 features that contain 90150 values matching 90150 sold parcelid, including parcelid. They are listed in tempF2. As latitude and longtitude are not necessary for this project, becuase we don't need to scope to the parcelid located in detail streets. Let us extract the data with other 14 fields with 90150 parcelid to explore firstly. 

With reference to the forum information supplied from Andrew Martin (Data Scientist at Zillow), https://www.kaggle.com/c/zillow-prize-1/discussion/34168, there are some duplicate data in the train16. Be careful to use it for calculation.

```{r}

tempF2df <- select(joinid, "parcelid", "bathroomcnt", "bedroomcnt", "roomcnt", "fips", "hashottuborspa", "propertycountylandusecode", "propertylandusetypeid", "propertyzoningdesc", "rawcensustractandblock", "regionidcounty", "taxdelinquencyflag") 

```

"hashottuborspa" and "taxdelinquencyflag" were not actually filled fully, but little. They don't meet the dataset narrow down policy mentioned above. Delete them.
"propertyzoningdesc" is "Description of the allowed land uses (zoning) for that property". It is also not filled fully. It is similar to "propertylandusetypeid" that is " Type of land use the property is zoned for". Delete "propertyzoningdesc".
"propertycountylandusecode" sounds similar to "propertylandusetypeid" and it has no more identification about its code in the dictionary. Delete it.

```{r}

ddply(tempF2df, .(fips), "nrow")

ddply(tempF2df, .(regionidcounty), "nrow") 

```

The results show that the properties from a same palce have the same fips code and regionidecounty.
fips = "6037", regionidcounty = "3101", Los Angeles County
fips = "6059", regionidcounty = "1286", Orange County
fips = "6111", regionidcounty = "2061", Ventura County

Therefore, delete "regionidcounty".

```{r}

ddply(tempF2df, .(propertylandusetypeid), "nrow") # check the usetype summaries

```

```{r}

tempF2df <- select(joinid, "parcelid", "bathroomcnt", "bedroomcnt", "roomcnt", "fips","propertylandusetypeid", "rawcensustractandblock") 

mertempF2df <- merge(x=tempF2df, y=train16, by = "parcelid", all.x = TRUE)# get the final tempF2df without missing data for estimation preparation.

```


# 1.4.4 Continue to narrow down tempF2df (seperate the parcelid in three counties: Los Angeles, Orange, Ventura)

As indicated above, the accuracy of Zestimate depands on the available data about the property and its location. Separate the individual month and county for training practice.

```{r}

LA <- subset(mertempF2df, fips == "6037") #Los Angeles County
OR <- subset(mertempF2df, fips == "6059") # Orange County
VE <- subset(mertempF2df, fips == "6111") #Ventura County

```



# 1.5 Analyse the data from three counties seperately

# Los Angeles


```{r}

tempLA1 <- ddply(LA, .(month), "nrow") 
names(tempLA1)[2] <- c("transactioncount")
tempLA1$month <- ordered(tempA$month, levels = c("December","November","October","September", "August", "July","June", "May", "April", "March",   "February","January" ))

ggplot(tempLA1) + geom_bar(aes(x= month, y=transactioncount), fill = "pink", stat = "identity") + xlab("Months") + ylab("Transaction Counts of Properties in LA (2016)")+ scale_y_continuous(limits = c(0, 11500), breaks = seq(0, 11500, 1000)) + geom_hline(yintercept = mean(tempLA1$transactioncount),linetype = "dashed", size = 2, color = "brown") + coord_flip() + theme_bw()
```



```{r,fig.width=10, fig.height=7}

tempLA2 <- ddply(LA, c("month"), summarise, mean = mean(logerror))
tempLA2$month <- ordered(tempLA2$month, levels = c("January", "February", "March","April","May", "June", "July","August","September",  "October","November", "December"))

ggplot(tempLA2, aes(x=month, y=mean, group=2)) + geom_point(color="brown", size=5) + geom_line(color="brown", size=1.5) + xlab ("Month (2016)") + ylab("Logerror (Brown) and Transaction Counts (Purple)") + scale_y_continuous(limits = c(0, 0.02), breaks = seq(0, 0.02, 0.002)) + geom_line(aes(x=tempLA1$month,y=tempLA1$transactioncount/10^6), color = "purple", size = 1.5) +theme_bw()

```

```{r,fig.width=10, fig.height=7}
tempLA3 <- merge(tempLA1, tempLA2, by = "month")

cor(tempLA3$mean, tempLA3$transactioncount)

```


```{r,fig.width=10, fig.height=7}

cor(LA$bathroomcnt, LA$logerror)

```
```{r,fig.width=10, fig.height=7}

cor(LA$bedroomcnt, LA$logerror)

```

```{r,fig.width=10, fig.height=7}

cor(LA$roomcnt, LA$logerror)

```

```{r,fig.width=10, fig.height=7}

cor(LA$propertylandusetypeid, LA$logerror)

```
```{r,fig.width=10, fig.height=7}

cor(LA$rawcensustractandblock, LA$logerror)

```

Comparing with the correlation between logeeror and all features in LA, transaction count shows stronger negative relationship with the logerror.










# 1.5 Relationships between the error and  taxvaluedollarcnt attributes


```{r,fig.width=10, fig.height=7}
ggplot(mergeid,aes(x=month)) + geom_boxplot(aes(y = mergeid$logerror), color="brown") + xlab ("Months (2016)") + ylab("Logerror Distribution")+ scale_y_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 0.25))+ geom_boxplot(aes(y = mergeid$taxvaluedollarcnt/10^7), color = "green")+ theme_bw()

```

```{r,fig.width=10, fig.height=7}
 


```



```{r,fig.width=10, fig.height=7}
cor(mergeid$logerror, mergeid$taxvaluedollarcnt/10^6)


```




# 2.0 Prediction Model

# 2.1 Split the train16 into two parts:

trainning data (from January to September)
test data (from October to December)


```{r}

train16_train <- subset(train16, month == "January" | month=="February"| month=="March" | month=="April"| month=="May"| month=="June" | month=="July"| month == "August"| month =="September")



train16_test <- subset(train16, month == "October" | month=="November"| month=="December")


install.packages("forecast")
library(forecast)

arma_fit <- auto.arima(train16_train) # NOT WORK. what is univariate time series?





```






# Others:




```{r}
oct16 <- subset(mergeid, month == "October", select = -transactiondate)
nov16 <- subset(mergeid, month== "November", select = -transactiondate)
dec16 <- subset(mergeid, month =="December", select = -transactiondate)

#   https://www.r-statistics.com/tag/aggregate/
```


## Other manipulation on the data


```{r,fig.width=10, fig.height=7}


library(ggmap)
CA <- get_map(location = "california", zoom = 6, scale = "auto", color = "bw", maptype = c("terrain"))
ggmap(CA)

```

# Zestimate predicts more accuracy on which kind of parcelid

```{r}
train16[,2] <- abs(train16$logerror)
library(ggplot2)
ggplot(train16, aes(logerror)) + geom_histogram(bins = 50) + scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,0.002))+ theme_bw()
tempC <- subset(train16, logerror < "0.05")

```

```{r}
install.packages("party")
library(party)
formula <- regionidzip ~ parcelid + fips + regionidcity + regionidcounty
ctreeA <- ctree(formula, data = prop16)# not allowed with missiong values
```


```{r}


```
