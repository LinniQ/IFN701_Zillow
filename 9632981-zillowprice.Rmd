 ---
title: "IFN701: Zillow's Home Value Prediction (Zestimate) --- Round 1 Logerrow Prediction"
author: "Linni, QIN n9632981"
output:
  pdf_document: default
  html_document: default
---

## Introduction

This R markdown file is going to find out the logerror of Zestimate in Oct. Nov. Dec. of both 2016 and 2017, which is associated to 3millions of properties in three counties in California, USA (LA, OR, VA). Logerror is the difference between the Zestimate's preicted market value and the actual sales price of the particular property. 

(Note: Detail introcution can be referred to Project Proposal)

Official data from Kaggle:
train_2016_v2.csv contatins the logerror of around 90thounds of properties sold within three specific counties through 2016.
properties_2016.csv includes the total 3millions of properties within three specific counties with common 57 attributes such as room numbers, square footage and locations.


# Documentations:

train16: a dataframe with importing train_2016_v2.csv (90275sold parcelid in 2016 (3% = 90275/2985217), logerror, transactiondate, + month)
  tempA: monthly transaction count based on train16
  tempB: monthly logerror mean based on train16
  tempAB: merging tempA and tempB, calculating the correlation between transaction counts and logerror mean

prop16: a dataframe with importing properties_2016.csv (57 features for 3millions of parcelid)

joinid: merging train16 and prop16 (90150 intersection parceldid with their 57 features)

mergeid: merging joinid and train16 (90275 parcelid with their 57 features and logerro, transactiondate, month)
tempM: missing data counts(58 features and their counter !na)
  tempM1: containing 32 features that missing data less than unitcnt(58353) (feature range for extended data exploartion)
    tempM2: among tempM1, there are 19 features containing data for all 90275 parcelid (correlation needed equal value frequency)
      tempM2df: extract the useful features based on exploring tempM2 from mergeid
          LA: LA1(transaction count) LA2(montly logerror mean) LA3 (LA1+LA2)
          OR:
          VE:


# Outcome:

1. data analysis report: find out the valuable data from the collected samples
eg.
    the pattern of the sales
    the pattern of the error distribution
    
    the parttern of missiong data
    --- 28 features contain more than 70% of data for parcelids
    --- 29 features contain less than 70% of data for parcelides (deleted)
    
    the factors to influcence the home value:
    --- transaction counts: cor (-0.7130957)
    
2. Prediction model
eg.
    decision tree/logistic regression to predict the logerror 
    

Prolem solving Methods:

...


# Call the useful packages

```{r}
library(ggplot2)

library(plyr)

#install.packages("dplyr")
library(dplyr)

install.packages("corrplot")
library(corrplot)

library(ggmap)

# for decision tree prediciton
install.packages("party")
install.packages("caret")
library(party)
library(caret)

```






# Import the available data

```{r,fig.width=10, fig.height=7}

train16 <- read.csv("./train_2016_v2.csv", header = TRUE) # "fread()" is a function to read large file and creat the table faster and more convenient.

prop16 <- read.csv("./properties_2016.csv", header = TRUE)
colnames(prop16)
```


## 1. Data Analysis


First of all, let's explore the Error rate, Transaction pattern and Logerror Distribution.


```{r}
# convert the date into the name of the month for further visulation analysis
train16$month <- months(as.Date(train16$transactiondate, format = "%d/%m/%y"))
head(train16)

```

# 1.1 The accuracy of Zestimate (https://www.zillow.com/zestimate/) 

    "Zillow's accuracy has a median error rate of 5%." 
    
    Is the median error value cloaser to their offical error rate?
    0.006 = 0.6% ?

```{r}

summarise(train16, Median=median(train16$logerror), Mean= mean(train16$logerror), Max=max(train16$logerror), Min=min(train16$logerror), Std=sd(train16$logerror)) 

```


# 1.2 Pattern of Transaction Counts of Zillow in 2016.

```{r}

tempA <- ddply(train16, .(month), "nrow") 
names(tempA)[2] <- c("transactioncount")
tempA$month <- ordered(tempA$month, levels = c("December","November","October","September", "August", "July","June", "May", "April", "March",   "February","January" ))


ggplot(tempA) + geom_bar(aes(x= month, y=transactioncount), fill = "pink", stat = "identity") + xlab("Months") + ylab("Transaction Counts of Properties on Zillow (2016)")+ scale_y_continuous(limits = c(0, 11500), breaks = seq(0, 11500, 1000)) + geom_hline(yintercept = mean(tempA$transactioncount),linetype = "dashed", size = 2, color = "brown") + coord_flip() + theme_bw()
```

The brown dashed line is the mean of the transaction time of Zillow in 2016. It can be learned from the diagram that it was a cold market from October to December. Their volume is far lower than the mean of transaction counts.  

# 1.3 Logerror Distribution pattern of Zillow in 2016

1.3.1 Distribution of Logerror


```{r,fig.width=10, fig.height=7}

train16$month <- ordered(train16$month, levels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October","November", "December"))

ggplot(train16) + geom_jitter(aes(x=month, y=logerror), stat = "identity", color = "brown") + xlab ("Months (2016)") + ylab("Logerror Distribution") + scale_y_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 0.25)) + geom_hline(yintercept = 0 ,linetype = "dashed", size = 2, color = "black")+ theme_bw()


```

```{r,fig.width=10, fig.height=7}
ggplot(train16, aes(logerror,fill = month)) + geom_histogram(bins = 50) + xlab ("Monthly Logerror Distribution of Zillow (2016)") + ylab("Count") + scale_y_continuous(limits = c(0, 7000), breaks = seq(0, 7000, 700)) + scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(-0.5,0.5,0.05))+ theme_bw()

```



1.3.2 Relationship between Monthly Logerror Mean and the Monthly Transaction Counts

```{r,fig.width=10, fig.height=7}

tempB <- ddply(train16, c("month"), summarise, mean = mean(logerror))
tempB$month <- ordered(tempB$month, levels = c("January", "February", "March","April","May", "June", "July","August","September",  "October","November", "December"))

# coef(lm(median ~ month, data = tempB ))

ggplot(tempB, aes(x=month, y=mean, group=1)) + geom_point(color="brown", size=5) + geom_line(color="brown", size=1.5) + xlab ("Month (2016)") + ylab("Logerror (Brown) and Transaction Counts (Purple)") + scale_y_continuous(limits = c(0, 0.02), breaks = seq(0, 0.02, 0.002)) + geom_line(aes(x=tempA$month,y=tempA$transactioncount/10^6), color = "purple", size = 1.5) +theme_bw()

```

As indicated as the above graph, the brown line represents the mean of error of each month and the purple line denotes the transaction times that is being sclaled by dividing 10^6. Considering the distribution of the error, the high logerrors happened among January, February, September, October, November and December of 2016 when the transaction times stayed low. A stable flutuation of the error happened from March to August when the transaction times stayed high. These two variables run in the opposite direction as shown. Let's check their correlation rate by using cor().


```{r,fig.width=10, fig.height=7}
tempAB <- merge(tempA, tempB, by = "month")

cor(tempAB$mean, tempAB$transactioncount/10^6)

```

In staticstics, the correlation is represented within the range of -1, 0 and 1. 0 indicates no correlation and 1 denotes a positive correlation.

The negative correlation as above between the logerror and the transaction numbers denotes while transaction number increases the logeeror decreases. (http://www.investopedia.com/terms/n/negative-correlation.asp)

Question: Can we use the transanction time to predict the error?

There is data of 2016 only for training. It is lack of strong evidence to prove that the transaction times stay low in the last season of every year. It means the error cannot be predicted based only on the transaction.



# 1.4  Coorelations among 57 home attributes 

According to Zillow, the accuracy of Zestimate depands on the available data supplied by the homeowners. So, does that mean:
January, February, September, October, November and December of 2016 = less home feature value?
March, April, May, June, July and August of 2016 = enough home feature value?
Let's combine prop16 and train16 by parcelid. The result file will tell us the physical features of the sold property in 2016 and their respective error. So, the relationships hidden among those factors might be disclosure.

# 1.4.1 Join property data and train data

```{r,fig.width=10, fig.height=10}

mergeid <- merge (x=train16, y=prop16, by = "parcelid", all.x = TRUE)

mergeidA <- select_if(mergeid,is.numeric) # correlationship is calculated with numeric value only
A <- cor(mergeidA) 
corrplot(A, method="circle")

```

if it needs to replace the name of months into numeric (they studio can regonize the name of the month as the integer number by itself) @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
```{r,fig.width=10, fig.height=10}

mergeid["month"] = "January" <- 1 
mergeid["month"] = "February" <- 2
mergeid["month"] = "March" <- 3
mergeid["month"] = "April" <- 4
mergeid["month"] = "May" <- 5
mergeid["month"] = "June" <- 6
mergeid["month"] = "July" <- 7
mergeid["month"] = "August" <- 8
mergeid["month"] = "September" <- 9
mergeid["month"] = "October" <- 10
mergeid["month"] = "November" <- 11
mergeid["month"] = "December" <- 12


```

The reason to caused the above question make is that there are missing value among different features. They are not equally filled with data. We need to explore the missing value and find the suitbale field to check their correlationships.


# 1.4.2 check the missing data for each field


```{r,fig.width=10, fig.height=20}
#sum(!is.na(mergeid$airconditioningtypeid)) # count the row number of column with values


#tempM1 <- ddply(mergeid, .(airconditioningtypeid), "nrow") # count row number for each unique value in the column

#apply(mergeid, 2, function(x) length(which(!is.na(x)))) # same results with colSums(!is.na(joinid))

tempM <- data.frame(colSums(!is.na(mergeid)))
names(tempM)[1] <- c("values")


ggplot(tempM,aes(x=reorder(rownames(tempM),-values), y=values)) + geom_bar( fill = "pink", stat = "identity") + xlab("Property Physical Features (2016)") + ylab("Available values Counts for Featrues")+ scale_y_continuous(limits = c(0, 91000), breaks = seq(0, 91000, 5000)) +coord_flip() + theme_bw()


```

```{r}
subset(tempM,rownames(tempM)=="unitcnt") #58353 

```
As we can see, from the top to unicnt, there are 29 (near 50%) property features that were supplied values for less than 58353 of the total amount of sold parcelids.

Let us narrow down the size of the dataset and focus on figuring out the relationship between the estimate error and the feature variables with full values.

# 1.4.3 Clean the data of joinid (90150 sold parcelids), out = tempF2dfn and correlationships between logerror and comparable 10 features

```{r}
tempM1 <- subset(tempM,tempM > 58353) 
nrow(tempM1) #32, including parcelid

```
There are 32 features that contain nealy full values matching 90275 sold parcelid, including parcelid. They are listed in tempM1.

```{r}

tempM2 <- subset(tempM,tempM == 90275) 
nrow(tempM2) #19, including parcelid

```

There are 19 features that contain 90275 values matching 90275 sold parcelid, including parcelid. They are listed in tempM2. As latitude and longtitude are not necessary for this project, becuase we don't need to scope to the parcelid located in detail streets. Let us extract the data with other 19 fields with 90275 parcelid to explore firstly. 

With reference to the forum information supplied from Andrew Martin (Data Scientist at Zillow), https://www.kaggle.com/c/zillow-prize-1/discussion/34168, there are some duplicate data in the train16. Be careful to use it for calculation.

```{r,fig.width=10, fig.height=7}

tempM2df <- select(mergeid, parcelid, logerror,transactiondate, month, bathroomcnt, bedroomcnt, fips, hashottuborspa,latitude, longitude,  propertycountylandusecode, propertylandusetypeid, propertyzoningdesc, rawcensustractandblock, regionidcounty, roomcnt,  fireplaceflag, assessmentyear, taxdelinquencyflag)

tempM2dfn <- select_if(tempM2df, is.numeric)
colnames(tempM2dfn) #12 feature (19-7) transactiondate, month,propertycountylandusecode,propertyzoningdesc,fireplaceflag, taxdelinquencyflag
B <- cor(tempM2dfn)
corrplot(B, method="circle")

```
```{r}

ddply(tempM2dfn, .(assessmentyear), "nrow")

```

```{r,fig.width=7, fig.height=5}

tempM2dfn <- tempM2dfn[,!(colnames(tempM2dfn) == "assessmentyear")]
colnames(tempM2dfn) #11 feature , delete "assessmentyear""
C <- cor(tempM2dfn)
corrplot(C, method="circle")

```

So, what we can learn from these two sections 1.3 and 1.4 is that logerror and transaction frequancy have negative correlation. However, it is difficult to see any correlationship between logerror and other features.

# Duplicated data

```{r}

ddply(tempM2dfn, .(fips), "nrow")

ddply(tempM2dfn, .(regionidcounty), "nrow") 

```

The results show that the properties from a same palce have the same fips code and regionidecounty.
fips = "6037", regionidcounty = "3101", Los Angeles County
fips = "6059", regionidcounty = "1286", Orange County
fips = "6111", regionidcounty = "2061", Ventura County

Therefore, delete "regionidcounty".
```{r}

tempM2dfn <- tempM2dfn[,!(colnames(tempM2dfn) == "regionidcounty")]
colnames(tempM2dfn) #10 feature , delete "regionidcounty""

```
# explore bathroomcnt (out: During 2016, the most popular parcelid is with 2 bathrooms, then 3 and 1)
```{r}

bath <-data.frame(ddply(tempM2dfn, .(bathroomcnt), "nrow"))
bath <- arrange(bath, desc(nrow))
print(bath)

ggplot(bath) + geom_bar(aes(x= bathroomcnt, y=nrow), fill = "pink", stat = "identity") + xlab("Bath Room Number") + ylab("Frequency")+ scale_y_continuous(limits = c(0, 37000), breaks = seq(0, 37000, 1000)) + scale_x_continuous(limits = c(-0.5, 12), breaks = seq(-0.5, 12, 0.5)) + theme_bw()

```



# explore bedroomcnt (out: During 2016, the most popular parcelid is with 3 bedrooms, then with 2 and 4 bedrooms)
```{r}

bed <-data.frame(ddply(tempM2dfn, .(bedroomcnt), "nrow"))
bed <- arrange(bed, desc(nrow))
print(bed)

ggplot(bed) + geom_bar(aes(x= bedroomcnt, y=nrow), fill = "pink", stat = "identity") + xlab("Bed Room Number") + ylab("Frequency")+ scale_y_continuous(limits = c(0, 36000), breaks = seq(0, 36000, 1000)) + scale_x_continuous(limits = c(-1, 15), breaks = seq(-1, 15, 1)) + theme_bw()

```



# explore propertylandusetypeid (out: the most popular property type id is 261 - Single Family Residential)
# (then 266 - Condominium) 
```{r}

pt <-data.frame(ddply(tempM2dfn, .(propertylandusetypeid), "nrow"))
pt <- arrange(pt, desc(nrow))
print(pt)

```

# explore fips (out: the most popular property is located in 6037 - LA, then 6059 - OR, last 6111 - VE)
```{r}

fips <-data.frame(ddply(tempM2dfn, .(fips), "nrow"))
fips <- arrange(fips, desc(nrow))
print(fips)

```


# explore room number (out: the most popular property is with 0 room - maybe the value is not supplied, then with 6 rooms and 7 rooms)
```{r}

room <-data.frame(ddply(tempM2dfn, .(roomcnt), "nrow"))
room <- arrange(room, desc(nrow))
print(room)

```


# Explore other features with data frequency between 58353 and 90275

# explore taxvaluedollarcnt = landtaxvaluedollarcnt + structuretaxvaluedollarcnt
#(out: tax value of the most popular property is between 300k and 500k)
#(the minimum tax value is 22, the maximum tax value is 27,750k)

```{r}

tax <-data.frame(ddply(mergeid, .(taxvaluedollarcnt), "nrow"))
tax <- arrange(tax, desc(nrow), desc(taxvaluedollarcnt))
tax[which.max(tax$taxvaluedollarcnt),]
tax[which.min(tax$taxvaluedollarcnt),]
print(tax)

```

# Explore the calculatedfinishedsquarefeet (out:the most popular property is without squarefeet data)
#(the second one is 1200sq.feet, the 3rd one is 1080sq.feet...)
# (with the results of the plotting image, the popular property is with sqaurefeet around 1200sq.feet=112sq.meter)
```{r}

feet <-data.frame(ddply(mergeid, .(calculatedfinishedsquarefeet), "nrow"))
feet <- arrange(feet, desc(nrow))
feet[which.max(feet$calculatedfinishedsquarefeet),]
feet[which.min(feet$calculatedfinishedsquarefeet),]
print(feet)
plot(feet)

```

# Explore yearbuilt (out: the most popular property is built in 1955)
#(top 30: 1950 - 1989)
# plot: popular wider range (1950 - 2007))
```{r}

year <-data.frame(ddply(mergeid, .(yearbuilt), "nrow"))
year <- arrange(year, desc(nrow))

print(year)
plot(year)

```

# Explore the parcelid (out: total sold: 90275, unique parcel id: 90150)
#(there are 125 parcelid were sold more than 2 times)
```{r}

id <-data.frame(ddply(mergeid, .(parcelid), "nrow"))
id <- arrange(id, desc(nrow))

print(id)
plot(id)

# explore the logeerors of the same parcelid (out: no regular development of the error. expecially for parcelid 11842707, it was sold 3 times repectively in July, August and September. Then, its logerror appears big gap based on the general distribution of the logerror)
a <- subset(mergeid, parcelid== "11842707" | parcelid=="10736972" |parcelid== "10790468")
print(a)

```


#### try to use decision tree model to prediction the logeeror with 11 above explored useful features 
#### divide the logerror into different 31 classes with interval 0.01 (range -0.15 --- +0.15  )


```{r}
p11 <- subset(mergeid, logerror < "-0.15" & logerror < "0") 
#confirm the subset results


p12 <- subset(mergeid, logerror == "0") 
p13 <- subset(mergeid, logerror > "0" &logerror < "0.15") 


p1 <- rbind(p11,p12,p13)
p1[which.max(p1$logerror),]
p1[which.min(p1$logerror),]

p1 <- select(p1, parcelid, logerror, month, bathroomcnt, bedroomcnt, fips, latitude, longitude,  propertylandusetypeid, roomcnt, taxvaluedollarcnt, calculatedfinishedsquarefeet, yearbuilt)

p1$taxvaluedollarcnt <- replace(p1$taxvaluedollarcnt, is.na(p1$taxvaluedollarcnt), 0)

p1$calculatedfinishedsquarefeet <- replace(p1$calculatedfinishedsquarefeet, is.na(p1$calculatedfinishedsquarefeet), 0)

p1$yearbuilt <- replace(p1$yearbuilt, is.na(p1$yearbuilt), 0)

p2 <- data.frame(colSums(!is.na(p1)))
names(p2)[1] <- c("values")

print(p2)


```
We can learn that it is 90% logerror between the range -0.15 - +0.15


# divide the logerror into their targeted classes and name it.@@@@@@@@@@@@@@@(skip this step if fips is classifier)

```{r}

p1$logerror <- replace(p1$logerror, p1$logerror <= "-0.14", 1)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.14" & p1$logerror <= "-0.13", 2)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.13" & p1$logerror <= "-0.12", 3)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.12" & p1$logerror <= "-0.11", 4)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.11" & p1$logerror <= "-0.10", 5)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.10" & p1$logerror <= "-0.09", 6)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.09" & p1$logerror <= "-0.08", 7)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.08" & p1$logerror <= "-0.07", 8)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.07" & p1$logerror <= "-0.06", 9)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.06" & p1$logerror <= "-0.05", 10)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.05" & p1$logerror <= "-0.04", 11)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.04" & p1$logerror <= "-0.03", 12)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.03" & p1$logerror <= "-0.02", 13)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.02" & p1$logerror <= "-0.01", 14)
p1$logerror <- replace(p1$logerror, p1$logerror > "-0.01" & p1$logerror <= "0", 0)
p1$logerror <- replace(p1$logerror, p1$logerror > "0" & p1$logerror <= "0.01", 15)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.01" & p1$logerror <= "0.02", 16)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.02" & p1$logerror <= "0.03", 17)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.03" & p1$logerror <= "0.04", 18)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.04" & p1$logerror <= "0.05", 19)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.05" & p1$logerror <= "0.06", 20)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.06" & p1$logerror <= "0.07", 21)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.07" & p1$logerror <= "0.08", 22)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.08" & p1$logerror <= "0.09", 23)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.09" & p1$logerror <= "0.10", 24)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.10" & p1$logerror <= "0.11", 25)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.11" & p1$logerror <= "0.12", 26)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.12" & p1$logerror <= "0.13", 27)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.13" & p1$logerror <= "0.14", 28)
p1$logerror <- replace(p1$logerror, p1$logerror > "0.14" & p1$logerror <= "0.15", 29)


```

# build the prediction model
## split the data into 70% of training and 30% of testing data

```{r}

p1_train <- subset(p1, month == "January" | month=="February"| month=="March" | month=="April"| month=="May"| month=="June" | month=="July"| month == "August"| month =="September") # 90% of total p1

p1_test <- subset(p1, month == "October" | month=="November"| month=="December") # 10% of total p1

```


# build formula ==> logerror as classifier (tree1)
```{r}

colnames(p1_train)
head(p1_train)

```

```{r,fig.width=30, fig.height=10}

formula <- logerror ~ parcelid + bathroomcnt + bedroomcnt + fips + latitude + longitude + propertylandusetypeid + roomcnt + taxvaluedollarcnt + calculatedfinishedsquarefeet + yearbuilt

summary(p1_train)

tree1 <- ctree(formula, data = p1_train)
plot(tree1)

```


```{r,fig.width=30, fig.height=10}

table(predict(tree1, newdata = p1_test),p1_test$logerror)

```



```{r,fig.width=30, fig.height=10}

confusionMatrix(predict(tree1, newdata = p1_test),p1_test$logerror) # not work

```

# build formula ==> fips as classifier (tree2)



```{r,fig.width=30, fig.height=10}

formula <- fips ~ parcelid+ bathroomcnt + bedroomcnt + logerror + latitude + longitude + propertylandusetypeid + roomcnt + taxvaluedollarcnt + calculatedfinishedsquarefeet + yearbuilt


tree2 <- ctree(formula, data = p1_train)
plot(tree2)

```




```{r,fig.width=30, fig.height=10}

confusionMatrix(predict(tree2, newdata = p1_test),p1_test$fips)

```

# build formula ==> propertylandusetypeid as classifier (tree3)



```{r,fig.width=30, fig.height=10}

formula <- propertylandusetypeid ~ parcelid + bathroomcnt + bedroomcnt + logerror + latitude + longitude + fips+ roomcnt + taxvaluedollarcnt + calculatedfinishedsquarefeet + yearbuilt


tree3 <- ctree(formula, data = p1_train)
plot(tree3)

```

```{r,fig.width=30, fig.height=10}

confusionMatrix(predict(tree3, newdata = p1_test),p1_test$propertylandusetypeid) # not work

```


As what has been discovered above, it has nearly 100% accurary prediction rate when setting the counties as the classifier. Given the testing dataset, it needs to do 7736 times of prediction. Then, 

the correct classification (True Positive + True Negative) of the matrix is 7735 (LA5113 OR1987 VE635)

There just is one error as the off-diagonal in the matrix (False Positive and False Negative).

Therefore the accuracy of Zestimate depands on the location of property.


## Multiple Linear Regression


```{r}

logerror.lm = lm(logerror ~ bathroomcnt + bedroomcnt + fips + latitude + longitude + propertylandusetypeid + roomcnt + taxvaluedollarcnt + calculatedfinishedsquarefeet + yearbuilt, data = p1)

coeffs1 = coefficients(logerror.lm); coeffs1
```

```{r}

subset(p1, p1$parcelid==10711755)
```


```{r}

i = -7.841951e-02 + 1.383555e-03*3 + (-1.600287e-03 * 3) + 5.133221e-05 * 6037 + (-4.451312e-09*34222040) + 1.265148e-09 * (-118622240) + 2.544108e-04*261 + -6.140825e-06 *0 + -5.989319e-10 * 459844 +  5.762484e-06 * 1589 + (-7.239128e-07) * 1959
print(i)
```

i is the predictive value for parcelid 10711755, but its actual logerror is -0.0182. The difference is 0.02047942. Regarding the the standard deviation of zillow's logerror (0.1610788), this difference is okay. 
```{r}

e = i - (-0.0182)
print(e)
```


# 1.4.4 Continue to narrow down tempM2dfn to check the logerror distribution for three counties (seperate the parcelid in three counties: Los Angeles, Orange, Ventura)


```{r}

LA <- subset(p1, fips == "6037") #Los Angeles County
OR <- subset(p1, fips == "6059") # Orange County
VE <- subset(p1, fips == "6111") #Ventura County

```


# 1.5 Analyse the data from three counties seperately

# Los Angeles


```{r,fig.width=10, fig.height=7}
tempLA1 <- ddply(LA, .(month), "nrow") 
names(tempLA1)[2] <- c("transactioncount")
tempLA1$month <- ordered(tempA$month, levels = c("December","November","October","September", "August", "July","June", "May", "April", "March",   "February","January" ))

ggplot(tempLA1) + geom_bar(aes(x= month, y=transactioncount), fill = "pink", stat = "identity") + xlab("Months") + ylab("Transaction Counts of Properties in LA (2016)")+ scale_y_continuous(limits = c(0, 11500), breaks = seq(0, 11500, 1000)) + geom_hline(yintercept = mean(tempLA1$transactioncount),linetype = "dashed", size = 2, color = "brown") + coord_flip() + theme_bw()
```



```{r,fig.width=10, fig.height=7}

tempLA2 <- ddply(LA, c("month"), summarise, mean = mean(logerror))
tempLA2$month <- ordered(tempLA2$month, levels = c("January", "February", "March","April","May", "June", "July","August","September",  "October","November", "December"))

ggplot(tempLA2, aes(x=month, y=mean, group=2)) + geom_point(color="brown", size=5) + geom_line(color="brown", size=1.5) + xlab ("Month (2016)") + ylab("Logerror (Brown) and Transaction Counts (Purple)") + scale_y_continuous(limits = c(0, 0.02), breaks = seq(0, 0.02, 0.002)) + geom_line(aes(x=tempLA1$month,y=tempLA1$transactioncount/10^6), color = "purple", size = 1.5) +theme_bw()

```

```{r,fig.width=10, fig.height=7}
tempLA3 <- merge(tempLA1, tempLA2, by = "month")

cor(tempLA3$mean, tempLA3$transactioncount)

cor(LA$bathroomcnt, LA$logerror)
plot(LA$bathroomcnt, LA$logerror)

```








```{r}
oct16 <- subset(mergeid, month == "October", select = -transactiondate)
nov16 <- subset(mergeid, month== "November", select = -transactiondate)
dec16 <- subset(mergeid, month =="December", select = -transactiondate)

#   https://www.r-statistics.com/tag/aggregate/
```



```{r,fig.width=10, fig.height=7}

CA <- get_map(location = "LA", zoom = 10, scale = "auto", color = "color", maptype = c("terrain"))
ggmap(CA)

```

