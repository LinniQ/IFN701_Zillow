 ---
title: "IFN701: Zillow's Home Value Prediction (Zestimate) --- Round 1 Logerrow Prediction"
author: "Linni, QIN n9632981"
output:
  pdf_document: default
  html_document: default
---

## Introduction

This R markdown file is going to find out the logerror of Zestimate in Oct. Nov. Dec. of both 2016 and 2017, which is associated to 3millions of properties in three counties in California, USA (LA, OR, VA). Logerror is the difference between the Zestimate's preicted market value and the actual sales price of the particular property. 

(Note: Detail introcution can be referred to Project Proposal)

Official data from Kaggle:
train_2016_v2.csv contatins the logerror of around 90thounds of properties sold within three specific counties through 2016.
properties_2016.csv includes the total 3millions of properties within three specific counties with common 57 attributes such as room numbers, square footage and locations.


# Documentations:

train16: a dataframe with importing train_2016_v2.csv (90275sold parcelid in 2016 (3% = 90275/2985217), logerror, transactiondate, + month)
  tempA: monthly transaction count based on train16
  tempB: monthly logerror mean based on train16
  tempAB: merging tempA and tempB, calculating the correlation between transaction counts and logerror mean

prop16: a dataframe with importing properties_2016.csv (57 features for 3millions of parcelid)

joinid: merging train16 and prop16 (90150 intersection parceldid with their 57 features)

mergeid: merging joinid and train16 (90275 parcelid with their 57 features and logerro, transactiondate, month)
tempM: missing data counts(58 features and their counter !na)
  tempM1: containing 32 features that missing data less than unitcnt(58353)
    tempM2: among tempM1, there are 19 features containing data for all 90275 parcelid
      tempM2df: extract the useful features based on exploring tempM2 from mergeid
          LA: LA1(transaction count) LA2(montly logerror mean) LA3 (LA1+LA2)
          OR:
          VE:


# Outcome:

1. data analysis report: find out the valuable data from the collected samples
eg.
    the pattern of the sales
    the pattern of the error distribution
    
    the parttern of missiong data
    --- 28 features contain more than 70% of data for parcelids
    --- 29 features contain less than 70% of data for parcelides (deleted)
    
    the factors to influcence the home value:
    --- transaction counts: cor (-0.7130957)
    
2. Prediction model
eg.
    decision tree/logistic regression to predict the logerror 
    

Prolem solving Methods:

...


# Call the useful packages

```{r}
install.packages("ggplot2")
library(ggplot2)
library(plyr)

#install.packages("dplyr")
library(dplyr)

#install.packages("corrplot")
library(corrplot)

```






# Import the available data

```{r,fig.width=10, fig.height=7}

train16 <- read.csv("./train_2016_v2.csv", header = TRUE) # "fread()" is a function to read large file and creat the table faster and more convenient.

prop16 <- read.csv("./properties_2016.csv", header = TRUE)
colnames(prop16)
```


## 1. Data Analysis


First of all, let's explore the Error rate, Transaction pattern and Logerror Distribution.


```{r}
# convert the date into the name of the month for further visulation analysis
train16$month <- months(as.Date(train16$transactiondate, format = "%d/%m/%y"))
head(train16)

```

# 1.1 The accuracy of Zestimate (https://www.zillow.com/zestimate/) 

    "Zillow's accuracy has a median error rate of 5%." 
    
    Is the median error value cloaser to their offical error rate?
    0.006 = 0.6% ?

```{r}


summarise(train16, Median=median(train16$logerror), Mean= mean(train16$logerror), Max=max(train16$logerror), Min=min(train16$logerror), Std=sd(train16$logerror)) 

```


# 1.2 Pattern of Transaction Counts of Zillow in 2016.

```{r}

tempA <- ddply(train16, .(month), "nrow") 
names(tempA)[2] <- c("transactioncount")
tempA$month <- ordered(tempA$month, levels = c("December","November","October","September", "August", "July","June", "May", "April", "March",   "February","January" ))


ggplot(tempA) + geom_bar(aes(x= month, y=transactioncount), fill = "pink", stat = "identity") + xlab("Months") + ylab("Transaction Counts of Properties on Zillow (2016)")+ scale_y_continuous(limits = c(0, 11500), breaks = seq(0, 11500, 1000)) + geom_hline(yintercept = mean(tempA$transactioncount),linetype = "dashed", size = 2, color = "brown") + coord_flip() + theme_bw()
```

The brown dashed line is the mean of the transaction time of Zillow in 2016. It can be learned from the diagram that it was a cold market from October to December. Their volume is far lower than the mean of transaction counts.  

# 1.3 Logerror Distribution pattern of Zillow in 2016

1.3.1 Distribution of Logerror


```{r,fig.width=10}

train16[which(train16$logerror== max(train16$logerror)),] # to determine the range of scale for later plotting
train16[which(train16$logerror== min(train16$logerror)),]

```

```{r,fig.width=10, fig.height=7}

train16$month <- ordered(train16$month, levels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October","November", "December"))

ggplot(train16) + geom_jitter(aes(x=month, y=logerror), stat = "identity", color = "brown") + xlab ("Months (2016)") + ylab("Logerror Distribution") + scale_y_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 0.25)) + geom_hline(yintercept = 0 ,linetype = "dashed", size = 2, color = "black")+ theme_bw()


```

```{r,fig.width=10, fig.height=7}
ggplot(train16, aes(logerror,fill = month)) + geom_histogram(bins = 50) + xlab ("Monthly Logerror Distribution of Zillow (2016)") + ylab("Count") + scale_y_continuous(limits = c(0, 7000), breaks = seq(0, 7000, 700)) + scale_x_continuous(limits = c(-0.5, 0.5), breaks = seq(-0.5,0.5,0.05))+ theme_bw()

```



1.3.2 Relationship between Monthly Logerror Mean and the Monthly Transaction Counts

```{r,fig.width=10, fig.height=7}

tempB <- ddply(train16, c("month"), summarise, mean = mean(logerror))
tempB$month <- ordered(tempB$month, levels = c("January", "February", "March","April","May", "June", "July","August","September",  "October","November", "December"))

# coef(lm(median ~ month, data = tempB ))

ggplot(tempB, aes(x=month, y=mean, group=1)) + geom_point(color="brown", size=5) + geom_line(color="brown", size=1.5) + xlab ("Month (2016)") + ylab("Logerror (Brown) and Transaction Counts (Purple)") + scale_y_continuous(limits = c(0, 0.02), breaks = seq(0, 0.02, 0.002)) + geom_line(aes(x=tempA$month,y=tempA$transactioncount/10^6), color = "purple", size = 1.5) +theme_bw()

```

As indicated as the above graph, the brown line represents the mean of error of each month and the purple line denotes the transaction times that is being sclaled by dividing 10^6. Considering the distribution of the error, the high logerrors happened among January, February, September, October, November and December of 2016 when the transaction times stayed low. A stable flutuation of the error happened from March to August when the transaction times stayed high. These two variables run in the opposite direction as shown. Let's check their correlation rate by using cor().


```{r,fig.width=10, fig.height=7}
tempAB <- merge(tempA, tempB, by = "month")

cor(tempAB$mean, tempAB$transactioncount/10^6)

```

In staticstics, the correlation is represented within the range of -1, 0 and 1. 0 indicates no correlation and 1 denotes a positive correlation.

The negative correlation as above between the logerror and the transaction numbers denotes while transaction number increases the logeeror decreases. (http://www.investopedia.com/terms/n/negative-correlation.asp)

Question: Can we use the transanction time to predict the error?

There is data of 2016 only for training. It is lack of strong evidence to prove that the transaction times stay low in the last season of every year. It means the error cannot be predicted based only on the transaction.



# 1.4  Coorelations among 57 home attributes 

According to Zillow, the accuracy of Zestimate depands on the available data supplied by the homeowners. So, does that mean:
January, February, September, October, November and December of 2016 = less home feature value?
March, April, May, June, July and August of 2016 = enough home feature value?
Let's combine prop16 and train16 by parcelid. The result file will tell us the physical features of the sold property in 2016 and their respective error. So, the relationships hidden among those factors might be disclosure.

# 1.4.1 Join property data and train data

```{r,fig.width=10, fig.height=7}


mergeid <- merge (x=train16, y=prop16, by = "parcelid", all.x = TRUE)

mergeid <- arrange(mergeid, month) # if descending: arrange(mergeid, desc(month))

mergeidA <- select_if(mergeid,is.numeric) # correlationship is calculated with numeric value only
A <- cor(mergeidA) 


```

```{r,fig.width=10, fig.height=10}

corrplot(A, method="circle")

```



# 1.4.2 check the missing data for each field


```{r,fig.width=10, fig.height=20}
#sum(!is.na(mergeid$airconditioningtypeid)) # count the row number of column with values


#tempM1 <- ddply(mergeid, .(airconditioningtypeid), "nrow") # count row number for each unique value in the column

#apply(mergeid, 2, function(x) length(which(!is.na(x)))) # same results with colSums(!is.na(joinid))

tempM <- data.frame(colSums(!is.na(mergeid)))
names(tempM)[1] <- c("values")


ggplot(tempM,aes(x=reorder(rownames(tempM),-values), y=values)) + geom_bar( fill = "pink", stat = "identity") + xlab("Property Physical Features (2016)") + ylab("Available values Counts for Featrues")+ scale_y_continuous(limits = c(0, 91000), breaks = seq(0, 91000, 5000)) +coord_flip() + theme_bw()


```

```{r}
subset(tempM,rownames(tempM)=="unitcnt") #58353 

```
As we can see, from the top to unicnt, there are 29 (near 50%) property features that were supplied values for less than 58353 of the total amount of sold parcelids.

Let us narrow down the size of the dataset and focus on figuring out the relationship between the estimate error and the feature variables with full values.

# 1.4.3 Clean the data of joinid (90150 sold parcelids), out = mertempF2df

```{r}
tempM1 <- subset(tempM,tempM > 58353) 
nrow(tempM1) #32, including parcelid

```
There are 32 features that contain nealy full values matching 90275 sold parcelid, including parcelid. They are listed in tempM1.

```{r}

tempM2 <- subset(tempM,tempM == 90275) 
nrow(tempM2) #19, including parcelid
```

There are 19 features that contain 90275 values matching 90275 sold parcelid, including parcelid. They are listed in tempM2. As latitude and longtitude are not necessary for this project, becuase we don't need to scope to the parcelid located in detail streets. Let us extract the data with other 19 fields with 90275 parcelid to explore firstly. 

With reference to the forum information supplied from Andrew Martin (Data Scientist at Zillow), https://www.kaggle.com/c/zillow-prize-1/discussion/34168, there are some duplicate data in the train16. Be careful to use it for calculation.

```{r}

tempM2df <- select(mergeid, "parcelid", "logerror","transactiondate", "month", "bathroomcnt", "bedroomcnt", "roomcnt", "fips", "hashottuborspa", "propertycountylandusecode", "propertylandusetypeid", "propertyzoningdesc", "rawcensustractandblock", "regionidcounty", "taxdelinquencyflag") 

```

After exploring, I found that:
"hashottuborspa" and "taxdelinquencyflag" were not actually filled fully, but little. They don't meet the dataset narrow down policy mentioned above. Delete them.
"propertyzoningdesc" is "Description of the allowed land uses (zoning) for that property". It is also not filled fully. It is similar to "propertylandusetypeid" that is " Type of land use the property is zoned for". Delete "propertyzoningdesc".
"propertycountylandusecode" sounds similar to "propertylandusetypeid" and it has no more identification about its code in the dictionary. Delete it.

```{r}

ddply(tempM2df, .(fips), "nrow")

ddply(tempM2df, .(regionidcounty), "nrow") 

```

The results show that the properties from a same palce have the same fips code and regionidecounty.
fips = "6037", regionidcounty = "3101", Los Angeles County
fips = "6059", regionidcounty = "1286", Orange County
fips = "6111", regionidcounty = "2061", Ventura County

Therefore, delete "regionidcounty".

```{r}

ddply(tempM2df, .(propertylandusetypeid), "nrow") # check the usetype summaries

```

```{r}

tempM2df <- select(mergeid, "parcelid","logerror","transactiondate", "month", "bathroomcnt", "bedroomcnt", "roomcnt", "fips","propertylandusetypeid", "rawcensustractandblock") 
plot(tempM2df$bathroomcnt, tempM2df$logerror)

```


```{r,fig.width=10, fig.height=7}

ggplot(tempM2df,aes(x=bathroomcnt, y=logerror)) + geom_jitter( colour = "brown") + xlab("Bathroom Number") + ylab("Logeeror")+ scale_y_continuous(limits = c(0, 0.02), breaks = seq(0, 0.02, 0.002)) +scale_x_continuous(limits = c(0,20), breaks = seq(0, 20, 1)) + theme_bw()



```

```{r,fig.width=10, fig.height=7}

cor(tempM2df$fips, tempM2df$logerror)
plot(tempM2df$fips, tempM2df$logerror)

```


```{r,fig.width=10, fig.height=7}

cor(tempM2df$bedroomcnt, tempM2df$logerror)
plot(tempM2df$bedroomcnt, tempM2df$logerror)

```


```{r,fig.width=10, fig.height=7}

cor(tempM2df$roomcnt, tempM2df$logerror)
plot(tempM2df$roomcnt, tempM2df$logerror)

```


```{r,fig.width=10, fig.height=7}

cor(tempM2df$rawcensustractandblock, tempM2df$logerror)
plot(tempM2df$rawcensustractandblock, tempM2df$logerror)

```


```{r,fig.width=10, fig.height=7}

cor(tempM2df$propertylandusetypeid, tempM2df$logerror)
plot(tempM2df$propertylandusetypeid, tempM2df$logerror)

```


```{r,fig.width=10, fig.height=7}
B1 <- select_if(mergeid,is.numeric)
B <- cor(tempM2df)
corrplot(B, method = "circle")

```

# 1.4.4 Continue to narrow down tempM2df (seperate the parcelid in three counties: Los Angeles, Orange, Ventura)

As indicated above, the accuracy of Zestimate depands on the available data about the property and its location. Separate the individual month and county for training practice.

```{r}

LA <- subset(tempM2df, fips == "6037") #Los Angeles County
OR <- subset(tempM2df, fips == "6059") # Orange County
VE <- subset(tempM2df, fips == "6111") #Ventura County

```



# 1.5 Analyse the data from three counties seperately

# Los Angeles


```{r,fig.width=10, fig.height=7}
tempLA1 <- ddply(LA, .(month), "nrow") 
names(tempLA1)[2] <- c("transactioncount")
tempLA1$month <- ordered(tempA$month, levels = c("December","November","October","September", "August", "July","June", "May", "April", "March",   "February","January" ))

ggplot(tempLA1) + geom_bar(aes(x= month, y=transactioncount), fill = "pink", stat = "identity") + xlab("Months") + ylab("Transaction Counts of Properties in LA (2016)")+ scale_y_continuous(limits = c(0, 11500), breaks = seq(0, 11500, 1000)) + geom_hline(yintercept = mean(tempLA1$transactioncount),linetype = "dashed", size = 2, color = "brown") + coord_flip() + theme_bw()
```



```{r,fig.width=10, fig.height=7}

tempLA2 <- ddply(LA, c("month"), summarise, mean = mean(logerror))
tempLA2$month <- ordered(tempLA2$month, levels = c("January", "February", "March","April","May", "June", "July","August","September",  "October","November", "December"))

ggplot(tempLA2, aes(x=month, y=mean, group=2)) + geom_point(color="brown", size=5) + geom_line(color="brown", size=1.5) + xlab ("Month (2016)") + ylab("Logerror (Brown) and Transaction Counts (Purple)") + scale_y_continuous(limits = c(0, 0.02), breaks = seq(0, 0.02, 0.002)) + geom_line(aes(x=tempLA1$month,y=tempLA1$transactioncount/10^6), color = "purple", size = 1.5) +theme_bw()

```

```{r,fig.width=10, fig.height=7}
tempLA3 <- merge(tempLA1, tempLA2, by = "month")

cor(tempLA3$mean, tempLA3$transactioncount)

cor(LA$bathroomcnt, LA$logerror)
plot(LA$bathroomcnt, LA$logerror)

```



```{r,fig.width=10, fig.height=7}
cor(LA$bedroomcnt, LA$logerror)
plot(LA$bedroomcnt, LA$logerror)


```

```{r,fig.width=10, fig.height=7}

cor(LA$roomcnt, LA$logerror)
plot(LA$roomcnt, LA$logerror)

```



```{r,fig.width=10, fig.height=7}

cor(LA$propertylandusetypeid, LA$logerror)
plot(LA$propertylandusetypeid, LA$logerror)
```



```{r,fig.width=10, fig.height=7}

cor(LA$rawcensustractandblock, LA$logerror)
plot(LA$rawcensustractandblock, LA$logerror)
```

So, logerror and transaction frequancy have negative correlation. (is linear relationsip?)
It is difficult to see any difference in error across the various 57 features. And features and logerror have no correlation. (but non-linear relationship?)





```{r,fig.width=10, fig.height=7}
z_vars <- colnames(mergeid)

corplot(cor(mergeid[, ..z_vars], use='pairwise.complete.obs'), type='lower')

```












# 2.0 Prediction Model for non-linear

# 2.1 Split the train16 into two parts:

trainning data (from January to September)
test data (from October to December)


```{r}

train16_train <- subset(train16, month == "January" | month=="February"| month=="March" | month=="April"| month=="May"| month=="June" | month=="July"| month == "August"| month =="September")



train16_test <- subset(train16, month == "October" | month=="November"| month=="December")


install.packages("forecast")
library(forecast)

arma_fit <- auto.arima(train16_train) # NOT WORK. what is univariate time series?





```






# Others:




```{r}
oct16 <- subset(mergeid, month == "October", select = -transactiondate)
nov16 <- subset(mergeid, month== "November", select = -transactiondate)
dec16 <- subset(mergeid, month =="December", select = -transactiondate)

#   https://www.r-statistics.com/tag/aggregate/
```


## Other manipulation on the data


```{r,fig.width=10, fig.height=7}


library(ggmap)
CA <- get_map(location = "california", zoom = 6, scale = "auto", color = "bw", maptype = c("terrain"))
ggmap(CA)

```

# Zestimate predicts more accuracy on which kind of parcelid

```{r}
train16[,2] <- abs(train16$logerror)
library(ggplot2)
ggplot(train16, aes(logerror)) + geom_histogram(bins = 50) + scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,0.002))+ theme_bw()
tempC <- subset(train16, logerror < "0.05")

```

```{r}
install.packages("party")
library(party)
formula <- regionidzip ~ parcelid + fips + regionidcity + regionidcounty
ctreeA <- ctree(formula, data = prop16)# not allowed with missiong values
```


```{r}


```
